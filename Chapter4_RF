## CHAPTER 4 - The impact of social and environmental extremes on cholera time varying reproduction number in Nigeria
## INCIDENCE, R, COVARIATE SELECTION, RANDOM FOREST, NOWCASTING, TRAFFIC-LIGHT SYSTEM  

# Load packages 
# R
library(incidence)
library(EpiEstim) 
library(sars2pack)
# Data manipulation
library(data.table)
library(plyr)
library(dplyr)
library(tibble)
# Date manipulation
library(zoo)
library(lubridate)
# Plotting
library(ggplot2)
library(magrittr)
# Model fitting
library(rsample) 
library(ranger)
library(caret) 


`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

## INCIDENCE & R BY STATE 

## Import my positive linelist data and just save the onset and state columns 
# Subset it 
df1 <- linelist_data[c(1,7)]
names(df1)[2]<-paste("date_of_onset")
onset18 <- subset(df1, date_of_onset <= "2018-12-31")
onset19 <- subset(df1, date_of_onset >= "2019-01-01")

# Save the states as a list 
# Run 18 and 19 separately because long gaps with no cases increases R uncertainty
adm_list <- (split(onset18, onset18$State))

# Subset out states with insufficient cases (40 case threshold) 
adm_list <- adm_list[sapply(adm_list, nrow)>=40] 
adm_data <- names(adm_list)

# Save serial interval and standard deviations for the sensitivity analysis 
mu1 <- 3
mu2 <- 5
mu3 <- 8
sd <- 8 

# Run function calculating Rt for each state over monthly sliding windows 
R_by_state <- function(adm_data) {
  i <- incidence(as.Date(adm_data$date_of_onset))
  t_start <- seq(2, nrow(i$count)-29) 
  t_end <- t_start + 29
  i <- data.frame(i$dates, i$counts)
  names(i)[1]<-paste("date")
  names(i)[2]<-paste("count")
  Rt <- estimate_Rt(i, estimation_family='epiestim', cumulative=TRUE, method = 'parametric_si', 
                    config = list(mean_si=mu2, std_si=sd, t_start = t_start, t_end = t_end))# change based on si
  i$t_start <- seq(nrow(i))
  Rt <- merge(i, Rt, by = "t_start", all.x = TRUE)
}
output_list18 <- lapply(adm_list, R_by_state)

## Do the same for 2019 then join them 
output_list <- Map(rbind, output_list18, output_list19)

# Merge to one data frame 
output_df <- ldply(output_list, data.frame)
output_df <- output_df[c(1,3,2,5,4,6,7,9,13)]
names(output_df)[1]<-paste("state")

# To remove the posterior CV warning message I ran states individually, so I could change t_start depending on their reported cases 
df1 <- df1[c(1,7)]
names(df1)[2]<-paste("date_of_onset")
df1$date_of_onset <- as.Date(df1$date_of_onset)
onset18 <- subset(df1, date_of_onset <= "2018-12-31")
onset19 <- subset(df1, date_of_onset >= "2019-01-01")
i18 <- onset18 %>% group_by(State, date_of_onset) %>% tally()
i19 <- onset19 %>% group_by(State, date_of_onset) %>% tally()
i18 <- i18 %>% group_by(State) %>% mutate(total = sum(n))
i19 <- i19 %>% group_by(State) %>% mutate(total = sum(n))
i18 <- subset(i18, total >= 40)
i19 <- subset(i19, total >= 40)

list2env(split(i18, i18$State), envir=.GlobalEnv)

d <- subset(onset18, State == "Adamawa")
i <- incidence(as.Date(d$date_of_onset))
t_start <- seq(2, nrow(i$count)-29) 
t_end <- t_start + 29
i <- data.frame(i$dates, i$counts)
names(i)[1]<-paste("date")
names(i)[2]<-paste("count")
Rt <- estimate_Rt(i, estimation_family='epiestim', cumulative=TRUE, method = 'parametric_si', 
                  config = list(mean_si=mu2, std_si=sd, t_start = t_start, t_end = t_end))# change based on si
names(Adamawa)[2]<-paste("date_start")
Adamawa <- merge(Adamawa, Rt, by = "date_start", all = TRUE)

list2env(split(i18, i18$State), envir=.GlobalEnv)
# "Adamawa" "Bauchi"  "Borno"   "Gombe"   "Katsina" "Yobe"   
# "Adamawa" "Borno" 

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

## COVARIATE SELECTION 
# Covariate selection is based on variable importance and correlation clustering 

# Correlation clustering 
# Read in data 
data1 <- read.csv("df1.csv") # file: "Hierarchical Regression + Random Forests/Data"
data1 <- data1[-c(1:3)]
data1 <- na.omit(data1)

corr1 <- hclust(dist(1-abs(cor(data1[-c(1)]))), method = 'average')
clusters <- cutree(hclust(dist(1-abs(cor(data1[-c(1)])))), k = 9)
rect.hclust(corr1, k = 9)
clusters2 <- data.frame(corr1[["labels"]], clusters)
clusters2 <- cutree(hclust(dist(1-abs(cor(data1[-c(1)])))), h = 0.25)

## Variable importance
set.seed(123)
split <- initial_split(data1, prop = .7)
train <- training(split)
test  <- testing(split)

OOB_RMSE <- vector(mode = "numeric", length = 100)

for(i in seq_along(OOB_RMSE)) {
  
  optimal_ranger <- ranger(
    formula         = Rt5 ~ ., 
    data            = train, 
    num.trees       = 500,
    mtry            = 2,
    min.node.size   = 5,
    sample.fraction = .7,
    importance      = 'impurity'
  )
  
  OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)

var_importance <- data.frame(optimal_ranger$variable.importance)
var_importance <- rownames_to_column(var_importance, "variable")
names(var_importance)[2]<-paste("x")

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

## RANDOM FOREST

# Find optimum parameters 
# Optimum ntree 
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry = c(sqrt(ncol(train))))
modellist <- list()

for (ntree in c(50,100,500,1000,1500,2000)){
  set.seed(123)
  fit <- train(Rt5~.,
               data = train,
               method = 'rf',
               metric = 'RMSE',
               tuneGrid = tunegrid,
               trControl = control,
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}

results <- resamples(modellist)
summary(results)
dotplot(results)
# Optimum = 100

# Optimum mtry 
control2 <- trainControl(method="boot", number=100)
boot <- train(Rt5 ~ ., data=train, trControl=control2, method="rf")
print(boot)
# Optimum = 2

# Fit best-fit model 
# Stepping through model possibilities, shown was the best fit accroding to RMSE, R2 and correlations 
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=5,
                        returnResamp = "final",
                        savePredictions = "final",
                        allowParallel = FALSE)
best_fit <- train(Rt5 ~ Sanitation + MPI + PDSI + monthly_conf, 
            data=train, 
            method='rf', 
            metric= "RMSE", 
            trControl=control)

# Test the model against the performance metrics 
pred_bestfit <- predict(best_fit, test)
# RMSE and R2
postResample(pred = pred_bestfit, obs = test$Rt5)
# MAE
mae <- mean(abs(predict(best_fit, test) - test$Rt5))
# Correlation 
cor(test$Rt5, pred_bestfit)

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

## NOWCASTING 

# Read in my 31 state R has not been calculated for  
filename <- "predictions_df6.xlsx"
read_excel_allsheets <- function(filename, tibble = FALSE) {
  sheets <- readxl::excel_sheets(filename)
  x <- lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  if(!tibble) x <- lapply(x, as.data.frame)
  names(x) <- sheets
  x
}

adm_list <- read_excel_allsheets("predictions_df6.xlsx")
adm_data <- c("Abia","Akwa_Ibom","Anambra","Bayelsa","Benue","Cross River","Delta","Ebonyi",
              "Edo","Ekiti","Enugu","FCT","Imo","Jigawa","Kaduna","Kano","Kebbi","Kogi",
              "Kwara","Lagos","Nasarawa","Niger","Ogun","Ondo","Osun","Oyo","Plateau",
              "Rivers","Sokoto","Taraba","Zamfara")
              
# Save my dates to identify data points later 
full_dates$YearMon <- as.yearmon(paste(full_dates$year, full_dates$month), "%Y %m")

# Create predictions using the best fit model for each state with CI  
state_R_preds <- function(adm_data) {
  preds <- predict(best_fit, data.frame(monthly_conf=adm_data$monthly_conf, 
                                        PDSI=adm_data$PDSI, 
                                        Sanitation=adm_data$Sanitation, 
                                        MPI=adm_data$MPI), 
                   interval = 'confidence')
}

output_list <- lapply(adm_list, state_R_preds)

# Add dates to the predictions 
add_dates <- function(adm_data) {
  adm_data <- cbind(adm_data, full_dates)
}

output_list <- lapply(output_list, add_dates)

output_list <- lapply(output_list, function(x) { x["Date"] <- NULL; x })
output_list <- lapply(output_list, function(x) { x["year"] <- NULL; x })
output_list <- lapply(output_list, function(x) { x["month"] <- NULL; x })

# Format the predictions to save 
remove_duplicates2 <- function(adm_data) {
  adm_data <- adm_data %>% group_by(YearMon) %>%
    mutate(fit2 = mean(fit))
  adm_data <- adm_data %>% group_by(YearMon) %>%
    mutate(lwr2 = mean(lwr))
  adm_data <- adm_data %>% group_by(YearMon) %>%
    mutate(upr2 = mean(upr))
}

output_list <- lapply(output_list, remove_duplicates2)

output_list <- lapply(output_list, function(x) { x["fit"] <- NULL; x })
output_list <- lapply(output_list, function(x) { x["upr"] <- NULL; x })
output_list <- lapply(output_list, function(x) { x["lwr"] <- NULL; x })

remove_duplicates <- function(adm_data) {
  adm_data <- adm_data %>% distinct()
}

output_list <- lapply(output_list, remove_duplicates)

write.xlsx(output_list, file = "R_preds_by_state.xlsx")

`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````

## TRAFFIC-LIGHT SYSTEM  

# From the dataset used to fit the model, split this by R over and under 1 
df2 <- df1 %>% mutate(R_threshold = case_when(
  Rt5 >= 1 ~ "Rt >= 1", 
  Rt5 <= 1 ~ "Rt < 1"
))

# Find all the mean values for R >= 1
mean(df2$PDSI)

R >= 1
Sanitation = 43.64327
PDSI = 0.2192958
Conflict = 7.152047
MPI =  0.3884532

# Find a range of values of each to predict 
range(df2$PDSI)

Range 
Sanitation = 30 70
PDSI = -4  4
Conflict = 0 40
MPI =  0.1 0.5

# For MPI
x <- seq(0.1,0.5, 0.02) 
y <- predict(best_fit, data.frame(Sanitation = rep(43.64327, 21),
                            PDSI = rep(0.2192958, 21), 
                            MPI = x,
                            monthly_conf = rep(7.152047, 21)))

y_mpi <- data.frame(x, y)
y_mpi <- y_mpi %>% mutate(R_threshold = case_when(
  y >= 1 ~ "Rt >= 1", 
  y <= 1 ~ "Rt < 1"
))

ggplot(y_conf, aes(x = R_threshold, y = x, color = R_threshold)) + 
         geom_point(size = 2) + 
  scale_color_manual(values = c("green","red")) + theme_bw() + coord_flip() + 
  labs(x = "Rt", y = "Multidimentional Poverty Index", 
       title = "MPI") + 
  guides(color = FALSE) + 
  theme(axis.text = element_text(face = "bold"),
        axis.title = element_text(face = "bold"),
        title = element_text(face = "bold", size = 12))
# Repeat for the other covariates 









